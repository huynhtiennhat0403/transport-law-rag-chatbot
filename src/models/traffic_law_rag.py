"""
RAG System cho Luáº­t Giao ThÃ´ng Viá»‡t Nam
Sá»­ dá»¥ng: ChromaDB + BGE-M3 + Groq (Llama 3.1)
"""

import chromadb
from sentence_transformers import SentenceTransformer
from groq import Groq
import os
from typing import List, Dict, Tuple
from pathlib import Path
class TrafficLawRAG:
    def __init__(
        self, 
        chromadb_path: str,
        collection_name: str = "law_traffic_vietnam",
        api_key: str = None
    ):
        """
        Khá»Ÿi táº¡o RAG system
        
        Args:
            chromadb_path: ÄÆ°á»ng dáº«n Ä‘áº¿n ChromaDB
            collection_name: TÃªn collection
            api_key: Groq API key (hoáº·c set biáº¿n mÃ´i trÆ°á»ng GROQ_API_KEY)
        """
        print("ğŸš€ Äang khá»Ÿi táº¡o Traffic Law RAG System...")
        
        # 1. Load ChromaDB
        print("ğŸ“‚ Äang load ChromaDB...")
        self.client = chromadb.PersistentClient(path=chromadb_path)
        self.collection = self.client.get_collection(name = collection_name)
        print(f"   âœ“ ÄÃ£ load collection '{collection_name}'")
        
        # 2. Load Embedding Model
        print("ğŸ”„ Äang load embedding model (BAAI/bge-m3)...")
        self.embedding_model = SentenceTransformer('BAAI/bge-m3')
        print("   âœ“ ÄÃ£ load embedding model")
        
        # 3. Initialize Groq Client
        print("ğŸ¤– Äang káº¿t ná»‘i Groq API...")
        if api_key is None:
            api_key = os.getenv("GROQ_API_KEY")
            if api_key is None:
                raise ValueError(
                    "Vui lÃ²ng cung cáº¥p GROQ_API_KEY!\n"
                    "CÃ¡ch 1: TrafficLawRAG(api_key='gsk_...')\n"
                    "CÃ¡ch 2: Set biáº¿n mÃ´i trÆ°á»ng GROQ_API_KEY"
                )
        
        self.llm_client = Groq(api_key=api_key)
        print("   âœ“ ÄÃ£ káº¿t ná»‘i Groq API")
        
        print("\nâœ… Khá»Ÿi táº¡o thÃ nh cÃ´ng! Sáºµn sÃ ng tráº£ lá»i cÃ¢u há»i.\n")
    
    def retrieve(self, query: str, n_results: int = 5) -> Dict:
        """
        TÃ¬m kiáº¿m cÃ¡c chunks liÃªn quan nháº¥t
        
        Args:
            query: CÃ¢u há»i ngÆ°á»i dÃ¹ng
            n_results: Sá»‘ lÆ°á»£ng chunks tráº£ vá»
            
        Returns:
            Dict chá»©a documents, metadatas, distances
        """
        # Encode query
        query_embedding = self.embedding_model.encode(
            query, 
            normalize_embeddings=True
        )
        
        # Query ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=n_results
        )
        
        return results
    
    def format_context(self, results: Dict) -> str:
        """
        Format cÃ¡c chunks thÃ nh context cho LLM
        
        Args:
            results: Káº¿t quáº£ tá»« ChromaDB
            
        Returns:
            String context Ä‘Ã£ Ä‘Æ°á»£c format
        """
        contexts = []
        
        for i, (doc, metadata) in enumerate(zip(
            results['documents'][0], 
            results['metadatas'][0]
        ), 1):
            ref = metadata.get('full_reference', 'N/A')
            contexts.append(f"[TrÃ­ch dáº«n {i}] {ref}\nNá»™i dung: {doc}")
        
        return "\n\n".join(contexts)
    
    def generate_answer(self, query: str, context: str, model: str = "llama-3.1-70b-versatile") -> str:
        """
        Táº¡o cÃ¢u tráº£ lá»i tá»« LLM
        
        Args:
            query: CÃ¢u há»i ngÆ°á»i dÃ¹ng
            context: Context tá»« retrieved chunks
            model: Model Groq (máº·c Ä‘á»‹nh: llama-3.1-70b-versatile)
                   CÃ¡c option khÃ¡c: mixtral-8x7b-32768, llama-3.1-8b-instant
            
        Returns:
            CÃ¢u tráº£ lá»i tá»« LLM
        """
        
        system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn tÆ° váº¥n Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ Viá»‡t Nam.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a CHÃNH XÃC trÃªn cÃ¡c quy Ä‘á»‹nh phÃ¡p luáº­t Ä‘Æ°á»£c cung cáº¥p
- TrÃ­ch dáº«n rÃµ rÃ ng Äiá»u, Khoáº£n, Äiá»ƒm liÃªn quan
- Giáº£i thÃ­ch dá»… hiá»ƒu, cÃ³ vÃ­ dá»¥ cá»¥ thá»ƒ náº¿u cáº§n
- Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§ hoáº·c khÃ´ng cÃ³ trong quy Ä‘á»‹nh, hÃ£y nÃ³i rÃµ

Äá»ŠNH Dáº NG TRáº¢ Lá»œI:
1. Tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i
2. TrÃ­ch dáº«n cá»¥ thá»ƒ: [Äiá»u X, Khoáº£n Y]
3. Giáº£i thÃ­ch chi tiáº¿t náº¿u cáº§n
4. LÆ°u Ã½ bá»• sung (náº¿u cÃ³)

LÆ¯U Ã:
- KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong quy Ä‘á»‹nh
- KHÃ”NG tráº£ lá»i vá» cÃ¡c váº¥n Ä‘á» ngoÃ i luáº­t giao thÃ´ng
- Sá»­ dá»¥ng ngÃ´n ngá»¯ lá»‹ch sá»±, chuyÃªn nghiá»‡p"""

        user_prompt = f"""CÃC QUY Äá»ŠNH LIÃŠN QUAN:

{context}

---

CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG:
{query}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c quy Ä‘á»‹nh trÃªn."""

        try:
            response = self.llm_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,  # Giáº£m Ä‘á»ƒ tÄƒng tÃ­nh chÃ­nh xÃ¡c
                max_tokens=1500,
                top_p=0.9
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            return f"âŒ Lá»—i khi gá»i Groq API: {str(e)}"
    
    def query(
        self, 
        question: str, 
        n_results: int = 5, 
        show_sources: bool = True,
        model: str = "llama-3.3-70b-versatile"
    ) -> Tuple[str, Dict]:
        """
        Truy váº¥n end-to-end: Retrieve â†’ Generate
        
        Args:
            question: CÃ¢u há»i ngÆ°á»i dÃ¹ng
            n_results: Sá»‘ chunks retrieve
            show_sources: CÃ³ hiá»ƒn thá»‹ nguá»“n trÃ­ch dáº«n khÃ´ng
            model: Model Groq sá»­ dá»¥ng
            
        Returns:
            Tuple (answer, retrieval_results)
        """
        
        print("="*80)
        print(f"â“ CÃ‚U Há»I: {question}")
        print("="*80)
        
        # Step 1: Retrieve
        print("\nğŸ” Äang tÃ¬m kiáº¿m quy Ä‘á»‹nh liÃªn quan...")
        results = self.retrieve(question, n_results)
        print(f"   âœ“ TÃ¬m tháº¥y {len(results['documents'][0])} quy Ä‘á»‹nh liÃªn quan")
        
        # Step 2: Format context
        context = self.format_context(results)
        
        # Step 3: Generate answer
        print("ğŸ¤– Äang táº¡o cÃ¢u tráº£ lá»i...\n")
        answer = self.generate_answer(question, context, model)
        
        # Display answer
        print("="*80)
        print("ğŸ’¡ TRáº¢ Lá»œI:")
        print("="*80)
        print(answer)
        print()
        
        # Display sources
        if show_sources:
            print("="*80)
            print("ğŸ“š NGUá»’N TRÃCH DáºªN:")
            print("="*80)
            for i, (metadata, distance) in enumerate(zip(
                results['metadatas'][0],
                results['distances'][0]
            ), 1):
                ref = metadata.get('full_reference', 'N/A')
                score = 1 - distance  # Convert distance to similarity score
                print(f"[{i}] {ref} (Ä‘á»™ liÃªn quan: {score:.2%})")
            print()
        
        return answer, results
    
    def batch_query(self, questions: List[str], **kwargs):
        """
        Truy váº¥n nhiá»u cÃ¢u há»i
        
        Args:
            questions: List cÃ¡c cÃ¢u há»i
            **kwargs: CÃ¡c tham sá»‘ cho query()
        """
        for i, q in enumerate(questions, 1):
            print(f"\n\n{'='*80}")
            print(f"CÃ‚U Há»I {i}/{len(questions)}")
            print(f"{'='*80}")
            
            self.query(q, **kwargs)
            
            if i < len(questions):
                print("\n" + "â”€"*80)


# ============================================
# DEMO USAGE
# ============================================

def main():
    """Demo script"""

    PROJECT_ROOT = Path(__file__).parent.parent.parent

    CHROMADB_PATH = PROJECT_ROOT / "data" / "law_chroma_db"

    rag = TrafficLawRAG(chromadb_path=CHROMADB_PATH.as_posix())
    
    # Test vá»›i nhiá»u cÃ¢u há»i
    test_questions = [
        "Äiá»u kiá»‡n Ä‘á»ƒ Ä‘Æ°á»£c lÃ¡i xe Ã´ tÃ´ lÃ  gÃ¬?",
        "NgÆ°á»i Ä‘i xe mÃ¡y cÃ³ báº¯t buá»™c Ä‘á»™i mÅ© báº£o hiá»ƒm khÃ´ng?",
        "Tá»‘c Ä‘á»™ tá»‘i Ä‘a trong khu dÃ¢n cÆ° lÃ  bao nhiÃªu?",
        "Uá»‘ng rÆ°á»£u bia cÃ³ Ä‘Æ°á»£c lÃ¡i xe khÃ´ng?",
        "Xe Ã´ tÃ´ cÃ³ báº¯t buá»™c pháº£i tháº¯t dÃ¢y an toÃ n khÃ´ng?"
    ]
    
    # Cháº¡y batch query
    rag.batch_query(
        test_questions,
        n_results=5,
        show_sources=True,
        model = "llama-3.3-70b-versatile"
    )
    
    print("\n" + "="*80)
    print("âœ… HOÃ€N THÃ€NH DEMO!")
    print("="*80)
    
    # Interactive mode
    print("\nğŸ’¬ Cháº¿ Ä‘á»™ tÆ°Æ¡ng tÃ¡c (gÃµ 'quit' Ä‘á»ƒ thoÃ¡t):\n")
    while True:
        question = input("â“ CÃ¢u há»i cá»§a báº¡n: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break
        
        if not question:
            continue
        
        rag.query(question, show_sources=True)
        print()


if __name__ == "__main__":
    main()